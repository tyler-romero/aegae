{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Low-Memory Dropout\n",
        "\n",
        "In this tutorial, you will write a memory-efficient implementation of dropout whose state\n",
        "will be composed of a single int32 seed. This differs from more traditional implementations of dropout,\n",
        "whose state is generally composed of a bit mask tensor of the same shape as the input.\n",
        "\n",
        "In doing so, you will learn about:\n",
        "\n",
        "* The limitations of naive implementations of Dropout with PyTorch.\n",
        "\n",
        "* Parallel pseudo-random number generation in Triton.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline\n",
        "\n",
        "The *dropout* operator was first introduced in [SRIVASTAVA2014]_ as a way to improve the performance\n",
        "of deep neural networks in low-data regime (i.e. regularization).\n",
        "\n",
        "It takes a vector as input and produces a vector of the same shape as output. Each scalar in the\n",
        "output has a probability $p$ of being changed to zero and otherwise it is copied from the input.\n",
        "This forces the network to perform well even when only $1 - p$ scalars from the input are available.\n",
        "\n",
        "At evaluation time we want to use the full power of the network so we set $p=0$. Naively this would\n",
        "increase the norm of the output (which can be a bad thing, e.g. it can lead to artificial decrease\n",
        "in the output softmax temperature). To prevent this we multiply the output by $\\frac{1}{1 - p}$, which\n",
        "keeps the norm consistent regardless of the dropout probability.\n",
        "\n",
        "Let's first take a look at the baseline implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------  ---------  --------  ---------  --------  --------  ---------  -------  ---------  -------  -------\n",
            "input      -0.217858  0.686082  -0.207928  0.21389   0.349987  -0.960543  -1.7569  -0.105131  1.10485  -1.0135\n",
            "keep mask   0         1          1         1         0          0          1        0         0         0\n",
            "output      0         1.37216   -0.415855  0.427781  0          0         -3.5138   0         0         0\n",
            "---------  ---------  --------  ---------  --------  --------  ---------  -------  ---------  -------  -------\n"
          ]
        }
      ],
      "source": [
        "import tabulate\n",
        "import torch\n",
        "\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def _dropout(\n",
        "    x_ptr,  # pointer to the input\n",
        "    x_keep_ptr,  # pointer to a mask of 0s and 1s\n",
        "    output_ptr,  # pointer to the output\n",
        "    n_elements,  # number of elements in the `x` tensor\n",
        "    p,  # probability that an element of `x` is changed to zero\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    pid = tl.program_id(axis=0)\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = offsets < n_elements\n",
        "    # Load data\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "    x_keep = tl.load(x_keep_ptr + offsets, mask=mask)\n",
        "    # The line below is the crucial part, described in the paragraph above!\n",
        "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
        "    # Write-back output\n",
        "    tl.store(output_ptr + offsets, output, mask=mask)\n",
        "\n",
        "\n",
        "def dropout(x, x_keep, p):\n",
        "    output = torch.empty_like(x)\n",
        "    assert x.is_contiguous()\n",
        "    n_elements = x.numel()\n",
        "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
        "    _dropout[grid](x, x_keep, output, n_elements, p, BLOCK_SIZE=1024)\n",
        "    return output\n",
        "\n",
        "\n",
        "# Input tensor\n",
        "x = torch.randn(size=(10, )).cuda()\n",
        "# Dropout mask\n",
        "p = 0.5\n",
        "x_keep = (torch.rand(size=(10, )) > p).to(torch.int32).cuda()\n",
        "#\n",
        "output = dropout(x, x_keep=x_keep, p=p)\n",
        "print(tabulate.tabulate([\n",
        "    [\"input\"] + x.tolist(),\n",
        "    [\"keep mask\"] + x_keep.tolist(),\n",
        "    [\"output\"] + output.tolist(),\n",
        "]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seeded dropout\n",
        "\n",
        "The above implementation of dropout works fine, but it can be a bit awkward to deal with. Firstly\n",
        "we need to store the dropout mask for backpropagation. Secondly, dropout state management can get\n",
        "very tricky when using recompute/checkpointing (e.g. see all the notes about `preserve_rng_state` in\n",
        "https://pytorch.org/docs/stable/checkpoint.html). In this tutorial we'll describe an alternative implementation\n",
        "that (1) has a smaller memory footprint; (2) requires less data movement; and (3) simplifies the management\n",
        "of persisting randomness across multiple invocations of the kernel.\n",
        "\n",
        "Pseudo-random number generation in Triton is simple! In this tutorial we will use the\n",
        ":code:`triton.language.rand` function which generates a block of uniformly distributed :code:`float32`\n",
        "values in [0, 1), given a seed and a block of :code:`int32` offsets. But if you need it, Triton also provides\n",
        "other `random number generation strategies<Random Number Generation>`.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Triton's implementation of PRNG is based on the Philox algorithm (described on [SALMON2011]_).</p></div>\n",
        "\n",
        "Let's put it all together.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------  -------  ---------  --------  ---------  ---------  --------  ------  -----------  --------  --------\n",
            "input                0.30761  -0.345031  -1.36491  -0.316972  -0.683191  0.196036  1.6587  -0.00541945  0.520476  -0.50679\n",
            "output (seed = 123)  0        -0.690062   0         0          0         0.392072  0        0           1.04095   -1.01358\n",
            "output (seed = 123)  0        -0.690062   0         0          0         0.392072  0        0           1.04095   -1.01358\n",
            "output (seed = 512)  0         0         -2.72983  -0.633944   0         0.392072  3.3174   0           0          0\n",
            "-------------------  -------  ---------  --------  ---------  ---------  --------  ------  -----------  --------  --------\n"
          ]
        }
      ],
      "source": [
        "@triton.jit\n",
        "def _seeded_dropout(\n",
        "    x_ptr,\n",
        "    output_ptr,\n",
        "    n_elements,\n",
        "    p,\n",
        "    seed,\n",
        "    BLOCK_SIZE: tl.constexpr,\n",
        "):\n",
        "    # compute memory offsets of elements handled by this instance\n",
        "    pid = tl.program_id(axis=0)\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "    # load data from x\n",
        "    mask = offsets < n_elements\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "    # randomly prune it\n",
        "    random = tl.rand(seed, offsets)\n",
        "    x_keep = random > p\n",
        "    # write-back\n",
        "    output = tl.where(x_keep, x / (1 - p), 0.0)\n",
        "    tl.store(output_ptr + offsets, output, mask=mask)\n",
        "\n",
        "\n",
        "def seeded_dropout(x, p, seed):\n",
        "    output = torch.empty_like(x)\n",
        "    assert x.is_contiguous()\n",
        "    n_elements = x.numel()\n",
        "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
        "    _seeded_dropout[grid](x, output, n_elements, p, seed, BLOCK_SIZE=1024)\n",
        "    return output\n",
        "\n",
        "\n",
        "x = torch.randn(size=(10, )).cuda()\n",
        "# Compare this to the baseline - dropout mask is never instantiated!\n",
        "output = seeded_dropout(x, p=0.5, seed=123)\n",
        "output2 = seeded_dropout(x, p=0.5, seed=123)\n",
        "output3 = seeded_dropout(x, p=0.5, seed=512)\n",
        "\n",
        "print(\n",
        "    tabulate.tabulate([\n",
        "        [\"input\"] + x.tolist(),\n",
        "        [\"output (seed = 123)\"] + output.tolist(),\n",
        "        [\"output (seed = 123)\"] + output2.tolist(),\n",
        "        [\"output (seed = 512)\"] + output3.tolist(),\n",
        "    ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Et Voil√†! We have a triton kernel that applies the same dropout mask provided the seed is the same!\n",
        "If you'd like explore further applications of pseudorandomness in GPU programming, we encourage you\n",
        "to explore the `python/triton/language/random.py`!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n",
        "\n",
        "1. Extend the kernel to operate over a matrix and use a vector of seeds - one per row.\n",
        "2. Add support for striding.\n",
        "3. (challenge) Implement a kernel for sparse Johnson-Lindenstrauss transform which generates the projection matrix on the fly each time using a seed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "\n",
        ".. [SALMON2011] John K. Salmon, Mark A. Moraes, Ron O. Dror, and David E. Shaw, \"Parallel Random Numbers: As Easy as 1, 2, 3\", 2011\n",
        ".. [SRIVASTAVA2014] Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov, \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\", JMLR 2014\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
